{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to Python (Part 3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7t2uZZ1Vk21"
      },
      "source": [
        "# Introduction to Python (Part 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbsH4WFxqgvh"
      },
      "source": [
        "## Load and explore the dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX-yUXm0OzP7"
      },
      "source": [
        "Modules in python are similar to packages in R. In order to use them you need to install them and then import them. The two easiest ways to install modules are using **pip install** and **conda install** followed by the name of the module. If you are unsure about the exact command and you use conda, I always advise to go to the https://anaconda.org/anaconda/ website and just search the module you need. You will find all information there!\n",
        "\n",
        "In order to import a module into python you have multiple options:\n",
        "\n",
        "1) You either import the full module by saying import [module name]\n",
        "\n",
        "2) You can also import only one function of the module by specifying: from [module name] import [function name]\n",
        "\n",
        "3) Usually when you import a module you give it a name: import numpy as np. In this way everytime you need to use functions from the numpy package you can call it with np"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDeyhqVm-mVA"
      },
      "source": [
        "# import breast cancer dataset from scikit-learn\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1BtlchEIlhj"
      },
      "source": [
        "# load dataset\n",
        "breast_data = load_breast_cancer().data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Yv8rP7KlnrH"
      },
      "source": [
        "Let's look at the shape of the dataset (how many columns and rows there are)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7XkqvSMIlkg"
      },
      "source": [
        "# check the shape of the data\n",
        "breast_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Oztxbp-IlnJ"
      },
      "source": [
        "# load the labels and check the shape\n",
        "breast_labels = load_breast_cancer().target\n",
        "breast_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZFNnIZjIlp5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# reshape the labels and concatenate the data and labels together\n",
        "# each row: data + label\n",
        "labels = np.reshape(breast_labels,(569,1))\n",
        "final_breast_data = np.concatenate([breast_data,labels],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGkPsMcYPRnN"
      },
      "source": [
        "In this chunk you can see that the function reshape belongs to the numpy module. Since we defined the numpy module as np, we can simply call the module by writing np.reshape.\n",
        "\n",
        "If you try to print final_breast_data, you will notice that it is really hard to understand and interpret. This is mainly due to the fact that when you print it, you can only see a set of arrays. It would be, however, much better and easier to have a dataframe!!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2mHY5b9mSA6"
      },
      "source": [
        "# Try to print the final_breast_data and see how it looks. Is it easy/difficult to understand?\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPzwjof3mRPe"
      },
      "source": [
        "\n",
        "Difficult right? Let's create a dataframe.\n",
        "\n",
        "In Python, in order to work with dataframes there is a specific module that must be used. This module is called **pandas**. Pandas can be a bit more tricky to use if compared to simple data manipulation in R, but it's really powerful. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM5TWbe9Ilsg"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# creat a dataframe from numpy array\n",
        "breast_dataset = pd.DataFrame(final_breast_data)\n",
        "\n",
        "# Try to check how the dataframe looks now. Is it better and easier to understand? \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-_C1rsfmng_"
      },
      "source": [
        "Can you see what is the problem with the current dataframe? The issue is that the column and row names don't have any name... what is the meaning of all these numbers??? In order to properly understand the dataframe, we should add column names.\n",
        "\n",
        "The information about the features contained in the dataframe is contained inside the feature_names of the data. Let's collect them in a variable called features and check them out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4LdG3IGIluo"
      },
      "source": [
        "# load feature names\n",
        "features = load_breast_cancer().feature_names\n",
        "features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2tc73hynE7T"
      },
      "source": [
        "Now that we have the features we just need to add them to the right columns. In pandas there is an easy way to add column names. Let's also give a common label to all these columns and call it features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVxwnKcDI4X7"
      },
      "source": [
        "# add column names to the dataframe\n",
        "breast_dataset.columns = np.append(features,'label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-NYzAvVnRrT"
      },
      "source": [
        "Now that you added the columns try to print the dataframe again. This time, try to print only the head or the tail of the dataframe. This can be useful when you have a lot of data and you don't need to check them all out. Printing a huge dataframe can be computationally expensive, and if not needed, it's always better to avoid it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAggVSk3I4as"
      },
      "source": [
        "# let's have a look of first five rows of the dataframe\n",
        "breast_dataset.head(5)\n",
        "\n",
        "# Try to print the first 10 rows of the data frame\n",
        "\n",
        "\n",
        "# Now try to print the last 10 rows (Tip: use tail)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVQiHDHXWzkb",
        "outputId": "81278193-49cf-41a3-d1a4-bb4291f9143b"
      },
      "source": [
        "'''\n",
        "For dataframe, if you want to select specific rows and columns, use \"dataFrame.loc[]\" with names and \"dataFrame.iloc[]\" with indices. \n",
        "In the parenthesis you first put the rows and then the columns. \n",
        "'''\n",
        "# Try to select the first element of the first row\n",
        "breast_dataset.loc[0, 'mean radius']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.99"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx5tnH0YZpMg"
      },
      "source": [
        "'''\n",
        "If you write \"dataFrame.loc[:, 'mean radius']\" you spoecify that you want to keep all the rows \n",
        "and columns with name \"mean radius\". \n",
        "'''\n",
        "# Try to select the first two columns\n",
        "breast_dataset.loc[:, ['mean radius', 'mean texture']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAy6u67ZnxYT"
      },
      "source": [
        "Now that we have our dataframe with all the features, we just need to apply some kind of normalization. Everytime you work with data, you always need to normalize them before doing any kind of analysis of visualization. \n",
        "\n",
        "A common way to normalize data is based on the mean and standard error. Python has a super nice module with a function that does it automatically for you, which is called StandardScaler. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSH27cXUI4dm"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# get all feature values and normalise the data\n",
        "x = breast_dataset.loc[:, features].values\n",
        "x = StandardScaler().fit_transform(x)\n",
        "\n",
        "# The reason why you want to keep only these columns is that these are the ones\n",
        "# you need to normalize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78OFJCAwI4fu"
      },
      "source": [
        "# check the shape of the data\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7zfgeQfpeS2"
      },
      "source": [
        "Since we normalized the data based on the mean and standard error, in order to check whether the normalization worked, we have to see whether the mean was scaled to 0 and the standard error to 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxCcognlI4jF"
      },
      "source": [
        "# check whether the data is normalised\n",
        "np.mean(x),np.std(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZc7BdHcpnuB"
      },
      "source": [
        "As you can see the normalization was successfull!!! Great job :) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnqSFzWvLU7n"
      },
      "source": [
        "## Visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIhMtXieLZkm"
      },
      "source": [
        "### Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T9g1VtxNAHr"
      },
      "source": [
        "Principal component analysis is a dimensionality reduction method. It is used to reduce the dimensions while, at the same time, try to keep most of the information. The new created components are obtained by maximizing the variance of the original dataset, with n features. PCA can be commonly used to visualize the data, by summarizing the n features into 2 principal components and by plotting them in the (x, y) coordinate space. You can potentially have as many pricipal components as the number of original features. But usually it is common to simply keep the firsts one, since they are the ones that explain most of the variance within the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xX9mk0BI4lj"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# project the thirty-dimensional Breast Cancer data to two-dimensional space\n",
        "pca_breast = PCA(n_components=2)\n",
        "principalComponents_breast = pca_breast.fit_transform(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKkvyPNyN5PT"
      },
      "source": [
        "A nice way to visualize the coordinatex established by the newly obtained PC is through a dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WFt3db-LR7W"
      },
      "source": [
        "# creat a dataframe of the projected data\n",
        "principal_breast_Df = pd.DataFrame(data = principalComponents_breast, columns = ['principal component 1', 'principal component 2'])\n",
        "principal_breast_Df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY2rl23AOwO6",
        "outputId": "6e459a5e-1d55-48d7-ba70-b84f9a7427b3"
      },
      "source": [
        "print('Explained variation per principal component: {}'.format(pca_breast.explained_variance_ratio_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Explained variation per principal component: [0.44272026 0.18971182]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoyRKCh-ODYr"
      },
      "source": [
        "As you can see, the first principal component explain almost 45 percent of the variance. Instread the second one explains already only 18 percent. The farther down you move from the first component, the least variance is explained, which means that the mroe information contained in the original features is loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYi6ttEyhRwb"
      },
      "source": [
        "# for loop through a list\n",
        "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
        "for i in fruits:\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTG9ZbHviKvX"
      },
      "source": [
        "# for loop through a string\n",
        "for i in \"fruits\":\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xguwc16ch05N"
      },
      "source": [
        "# for loop through a range\n",
        "for i in range(5):\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BEQRlgnRrk1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6_YqKPiOUHY"
      },
      "source": [
        "Matplotlib is a common python module to create plots in Python. You can decribe it as the python \"ggplot\". It is really straighforward to use. You always start the command with plt. (which is the way you usually name the module when you import it) and then add directly after the command you need. \n",
        "\n",
        "The first thing to do define the size of the figure, the labels names and sizes, the title, the you can plot concretely your image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4lQMWR6V69Y"
      },
      "source": [
        "# plot the 2D PCA projection\n",
        "# Establish the dimensions of the figure you want to plot\n",
        "plt.figure(figsize=(10,10))\n",
        "# Specify the font of the x and y ticks\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=14)\n",
        "# Specify the x and y labels\n",
        "plt.xlabel('Principal Component - 1',fontsize=20)\n",
        "plt.ylabel('Principal Component - 2',fontsize=20)\n",
        "# Give a title\n",
        "plt.title(\"Principal Component Analysis of Breast Cancer Dataset\",fontsize=20)\n",
        "# Specify targets and colors (you want to color the points based on the condition)\n",
        "targets = [0, 1]\n",
        "colors = ['r', 'g']\n",
        "for target, color in zip(targets,colors):\n",
        "    # Keep the data that are either healthy or malign (have either target 0 or 1)\n",
        "    indicesToKeep = breast_dataset['label'] == target\n",
        "    # Plot a scatteplot where the X is the PC1 and y is PC1\n",
        "    plt.scatter(principal_breast_Df.loc[indicesToKeep, 'principal component 1'], \n",
        "                principal_breast_Df.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)\n",
        "# Plot the legend\n",
        "plt.legend(['Benign', 'Malignant'],prop={'size': 15})\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9jdEndQPreK"
      },
      "source": [
        "As you can see from the plot, the two principal components can split very well the samples that are benign and malign. This means two things: the samples are clearly distinct between each other, and the principal components obtained could summarize well the n features of the original data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVTdvwn4q0HB"
      },
      "source": [
        "If you want to play a bit with the plot, you can try to change the colors, titles, sizes, labels, legends .... Let us know if you don't understand something, we are here to help!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM4xQGWqXXKG"
      },
      "source": [
        "### t-distributed Stochastic Neighbor Embedding (t-SNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDtr6bU9rfZr"
      },
      "source": [
        "Principal component analysis is a nice visualization method, but it is linear. That's why sometimes it cannot fully captures all the information contained in the data. On the other hand, t-sne is a really nice, easy to use, visualization method.\n",
        "\n",
        "Disadvantage: it is stochastic so the plot could change every time you run the code!!! Also, if you change the perplexity variable the plot will look differently. How do you choose the right perplexity? That's a good question, for which there is no good answer. I would say... Try more values and see what you get!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCkuZkpuY6MJ"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne_breast = TSNE(n_components=2, perplexity=30).fit_transform(x)\n",
        "\n",
        "# plot the 2D t-SNE projection\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.xlabel('t-SNE - 1',fontsize=20)\n",
        "plt.ylabel('t-SNE - 2',fontsize=20)\n",
        "plt.title(\"t-distributed Stochastic Neighbor Embedding of Breast Cancer Dataset\",fontsize=20)\n",
        "targets = [0, 1]\n",
        "colors = ['r', 'g']\n",
        "for target, color in zip(targets,colors):\n",
        "    indicesToKeep = breast_dataset['label'] == target\n",
        "    plt.scatter(tsne_breast[indicesToKeep, 0], tsne_breast[indicesToKeep, 1], c = color, s = 50)\n",
        "\n",
        "plt.legend(['Benign', 'Malignant'],prop={'size': 15})\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJsn8BR3bMhx"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO5o0VKaBC3U"
      },
      "source": [
        "Classification is commonly used to understand whether the data you have can distinguish between two or more classes. This is a type of machine learning (together with regression). In order to do machine learning, you always have to split your data into training and test set. The training set is used to train the model, and the test one is used to evaluated the recently trained model on unseen data. \n",
        "\n",
        "It is common in machine learning to do a 5-fold cross validation. This consists in dividing your data into 5 groups, then select a train and test set per group and finally train a model using each training set. In the end, then, you would have 5 different trained models. \n",
        "\n",
        "This is commonly done to prove that your results are really significant and not due to chance, or to select hyperparameters. \n",
        "\n",
        "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl1O0wC8R0B4"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "acc = 0\n",
        "\n",
        "# get training set indices and testing set indices\n",
        "for train_index, test_index in kf.split(x):\n",
        "    \n",
        "    X_train, X_test = x[train_index], x[test_index]\n",
        "    y_train, y_test = breast_dataset['label'][train_index], breast_dataset['label'][test_index]\n",
        "\n",
        "    # fit a 5 nearest neighbour model with training set\n",
        "    clf_knn = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n",
        "    # predict the labels for testing set\n",
        "    y_pred = clf_knn.predict(X_test)\n",
        "    # calculate the accuracy for this model\n",
        "    acc += accuracy_score(y_test, y_pred)/5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhLKYMxFR0E-"
      },
      "source": [
        "# 5-fold cross validation accuracy\n",
        "acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkh4VMQUW6pJ"
      },
      "source": [
        "### References\n",
        "1. https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python\n",
        "2. https://www.datacamp.com/community/tutorials/introduction-t-sne\n",
        "3. https://scikit-learn.org/stable/_images/grid_search_cross_validation.png"
      ]
    }
  ]
}